{"cells":[{"cell_type":"markdown","source":["## The Problem\n<br>\nA very common case is to process files in a folder. When processing files we only want to process the same file once. There are different ways to solve this:\n- Process and then move/delete if successfull. \n- Keeping a list of metadata of all processed files and other ways.\n- React to file system events when a new file arrives and put the event on a queue that we consume\n\nAutoloader is using the last approach mentioned above combined with streaming and checkpoints to make things more easy.<br>\n\n**This notebook will show how you can use Autoloader and how to use the two different modes of Autoloader in Azure.**"],"metadata":{}},{"cell_type":"markdown","source":["###Auto Loader\nIncrementally and efficiently processes new data files as they arrive in Azure Blob storage or Azure Data Lake Storage Gen2 without any additional setup. Auto Loader provides a new Structured Streaming source called *cloudFiles*. Given an input directory path on the cloud file storage, the cloudFiles source automatically processes new files as they arrive, with the option of also processing existing files in that directory.\n\nDocs\n\n\nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader<br>\nhttps://databricks.com/blog/2020/02/24/introducing-databricks-ingest-easy-data-ingestion-into-delta-lake.html"],"metadata":{}},{"cell_type":"markdown","source":["There are two types of file listening:\n* Directory listening -  (for few files every day) - simple to get started with\n* File notification - (For many files and directories) - using event grid and storage queues (requires extra permission setup)\n\nSpecify:\n\n<code>.option(\"cloudFiles.useNotifications\", \"true\")</code>\n\nto use File notification listening.\n\nYou can change mode when you restart the stream. For example, you may want to switch to file notification mode when the directory listing is getting too slow due to the increase in input directory size. For both modes, Auto Loader internally keeps tracks of what files have been processed to provide exactly-once semantics, so you do not need to manage any state information yourself."],"metadata":{}},{"cell_type":"markdown","source":["Example synax:<br>\n*If you have data coming only once every few hours, you can still leverage auto loader in a scheduled job using Structured Streaming’s Trigger.Once mode.*\n```python\ndf = spark.readStream.format(\"cloudFiles\")\n  .option(<cloudFiles-option>, <option-value>)\n  .schema(<schema>)\n  .load(<input-path>)\n\ndf.writeStream.format(\"delta\")\n  .option(\"checkpointLocation\", <checkpoint-path>)\n  .start(<output-path>)\n\n# Example\nval df = spark.readStream.format(\"cloudFiles\")\n     .option(\"cloudFiles.format\", \"json\")\n         .load(\"/input/path\")\n\ndf.writeStream.trigger(Trigger.Once)\n         .format(“delta”)\n         .start(“/output/path”)\n```"],"metadata":{}},{"cell_type":"markdown","source":["For <code>cloudFiles-option</code> see:\nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader#configuration for options."],"metadata":{}},{"cell_type":"markdown","source":["Below exmaple is based on having an inbox folder with files in parquet format.\nWe will also write to a new location as we consume new files in the inbox. We will append data."],"metadata":{}},{"cell_type":"code","source":["# Setup\npathToInbox = \"/mnt/datasetsneugen2/Autoloader/dataset1\"\npathToOutputAppend = \"/tmp/afterAutoLoader/dataset1\"\npathToSchemForPaqrquet = \"/tmp/autoloader/inbox\"\npathToCheckpoint = \"/checkpoints/Autoloader/dataset1\""],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Cleanup - remove inbox output\ndbutils.fs.rm(pathToInbox, True)\ndbutils.fs.rm(pathToOutputAppend, True)\n# Only delete checkpoint first time (if needed)\n# dbutils.fs.rm(pathToCheckpoint, True)\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[38]: True</div>"]}}],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.functions import lit,col,concat, substring\n\n# Utility function to create new files in inbox\ndef createParquetFile(path, noRows = 10000):\n  dfIds = spark.range(noRows) #.withColumn(\"IDAsString\", concat(lit(\"Str=\"), col(\"id\")))\n  dfIds.write.mode(\"append\").parquet(path)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Create a firts file\ncreateParquetFile(pathToInbox)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Define source schema\n# read schema from existing parquet files in mounted data lake\ndfSchema = spark.read.parquet(pathToSchemForPaqrquet)\nschemaParquet = dfSchema.schema\n# or you can define you own schema by hand. (Faster to read schema from file if we have it though)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["# Or manual schema\n#from pyspark.sql.types import StructType, StructField, LongType\n\n# schemaParquet = StructType([\n#   StructField(\"id\", LongType(), True)\n#])\n# schemaParquet\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: StructType(List(StructField(id,LongType,true)))</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Start test with directory listening without extra permission setup\n# Good for few files and folders on a regular basis\n# We are monitoring folder /mnt/datasetsneugen2/Autoloader/dataset1 for new files and folders\n\ndf = spark.readStream.format(\"cloudFiles\")\\\n.option(\"cloudFiles.format\", \"parquet\")\\\n.option(\"cloudFiles.includeExistingFiles\", \"false\")\\\n.option(\"cloudFiles.useNotifications\", \"false\")\\\n.schema(schemaParquet)\\\n.load(pathToInbox)\n\n# The key here is to use .trigger(once=True) so that you can use the capabilities with streaming using checkpoints,\n# but run the streaming in batches whenever you want based on a schedule.\n# We are writing new data to /tmp/afterAutoLoader/dataset1\ndf.writeStream\\\n.trigger(once=True)\\\n.format(\"delta\")\\\n.outputMode(\"append\")\\\n.option(\"checkpointLocation\", pathToCheckpoint)\\\n.start(pathToOutputAppend)\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[46]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f539a9859d0&gt;</div>"]}}],"execution_count":13},{"cell_type":"code","source":["display(dbutils.fs.ls(pathToOutputAppend))"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/tmp/afterAutoLoader/dataset1/_delta_log/</td><td>_delta_log/</td><td>0</td></tr><tr><td>dbfs:/tmp/afterAutoLoader/dataset1/part-00000-d8b01e1b-1bda-4f9e-923b-4cf3d55b1a22-c000.snappy.parquet</td><td>part-00000-d8b01e1b-1bda-4f9e-923b-4cf3d55b1a22-c000.snappy.parquet</td><td>308</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"code","source":["spark.read.format(\"delta\").load(pathToOutputAppend).count()\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[47]: 10000</div>"]}}],"execution_count":15},{"cell_type":"code","source":["# Drop a file or 2 in the \"inbox\" folder and run the code again above (Trigger once)\n# or run script below to copy"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["createParquetFile(pathToInbox)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["#### Change to File notification\nWhen you need to scale up you can change to File Notification mode instead.<br>\nDatabricks will make the change easy and remember what files we already have read when using the Directory listening.<br>\n\nWe now change from directory listening to File Notification mode.\nThis mode will use Event Grid to subscribe to event in the folder we chose. A Storage queue will be used to store the events.\nThis solution is more scalable, but requires some extra setup in Azure.\n\nYou must provide the following authentication options only if you choose file notification mode (cloudFiles.useNotifications = true):\n\n|Authentication Option|Type|Default|Description|\n|---------------------|----|-------|-----------|\n|cloudFiles.connectionString|String|None|The connection string for the storage account, based on either account access key or shared access signature (SAS)|\n|cloudFiles.resourceGroup|String|None|The Azure Resource Group under which the storage account is created|\n|cloudFiles.subscriptionId|String|None|The Azure Subscription ID under which the resource group is created|\n|cloudFiles.tenantId|String|None|The Azure Tenant ID under which the service principal is created|\n|cloudFiles.clientId|String|None|The client ID or application ID, of the service principal|\n|cloudFiles.clientSecret|String|None|The client secret of the service principal|\n|cloudFiles.queueName|String|None|The URL of the Azure queue. If provided, the cloud files source directly consumes events from this queue instead of setting up its own Azure Event Grid and Queue Storage services. In that case, your cloudFiles.connectionString requires only read permissions on the queue.|"],"metadata":{}},{"cell_type":"code","source":["# SP/App ID from keyvault\nappId = dbutils.secrets.get(scope = \"databricks\", key = \"datasetneugen2-sp-app-id\")\n# SP/app secret from key vault\nappSecret = dbutils.secrets.get(scope = \"databricks\", key = \"datasetneugen2-sp-secret\")"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["# NOTE IMPORTANT - Permissions\n# In the portal:\n# Add the SP via its name (in my case databricksneu) as EventGrid EventSubscription Contributor (on IAM on the subscription)\n# Add the SP via its name (in my case databricksneu) as Contributor for the storage account where we want to listen for new files\n\n# Databricks will create a \"Event Grid System Topics\" with a storage que as the endpoint (our account)\n\n# Note: If you want to create our own topic and endpoint queue yourself you can give the name of the storage account and the queue instead of letting databricks create this for you."],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["createParquetFile(pathToInbox)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["# Get connection string from Key Vault for contributor right on storage\nconStr = dbutils.secrets.get(scope = \"databricks\", key = \"datasetsneugen2-contributor-con-str\")\nsubscriptionId = dbutils.secrets.get(scope = \"databricks\", key = \"subscriptionId\")\ntenantId = dbutils.secrets.get(scope = \"databricks\", key = \"tenantId\")\n\ndf = spark.readStream.format(\"cloudFiles\")\\\n.option(\"cloudFiles.connectionString\", conStr)\\\n.option(\"cloudFiles.resourceGroup\",\"datasetsNEUGen2\")\\\n.option(\"cloudFiles.subscriptionId\",subscriptionId)\\\n.option(\"cloudFiles.tenantId\",tenantId)\\\n.option(\"cloudFiles.clientId\", appId)\\\n.option(\"cloudFiles.clientSecret\",appSecret)\\\n.option(\"cloudFiles.format\", \"parquet\")\\\n.option(\"cloudFiles.includeExistingFiles\", \"false\")\\\n.option(\"cloudFiles.useNotifications\", \"true\")\\\n.schema(schemaParquet)\\\n.load(pathToInbox)\n\n#df.writeStream.trigger(Trigger.Once)\ndf.writeStream\\\n.trigger(once=True)\\\n.format(\"delta\")\\\n.outputMode(\"append\")\\\n.option(\"checkpointLocation\", pathToCheckpoint)\\\n.start(pathToOutputAppend)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[56]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f539a972e10&gt;</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["You can look at the back end of this in the portal:<br>\nhttps://ms.portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.EventGrid%2FsystemTopics<br>\ntoo see that an \"Event Grid System Topic\" is created that will route the event to a StorageQueue. The names of the \"Event Grid System Topic\" and the StorageQueue are prefixed with databricks- so you can find them.\n\n**Event grid system topic**<br>\n![Event Grid System Topic](https://datamh.blob.core.windows.net/public/img/EventGridSystemTopic.png)\n<br>\n**and the details of the Event Grid System Topic:**<br>\n![Event Grid System Topic Details](https://datamh.blob.core.windows.net/public/img/EventGridSystemTopicDetails.png)\n<br>\nThis topic is created by Databricks automatically using the contributors right on the Eventgrid. As it says in the docs above:<br>\n- You can create your on Storage Queue and create your own topic. In that case Databricks only needs to have read access to the queue."],"metadata":{}},{"cell_type":"code","source":["# Test to drop some new files in our inbox foler and re-run the above cell."],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["createParquetFile(pathToInbox, 500)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["# Check if we got more data\nspark.read.format(\"delta\").load(pathToOutputAppend).count()"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[57]: 20500</div>"]}}],"execution_count":26}],"metadata":{"name":"Autoloader Example v2","notebookId":2512024850521632},"nbformat":4,"nbformat_minor":0}
