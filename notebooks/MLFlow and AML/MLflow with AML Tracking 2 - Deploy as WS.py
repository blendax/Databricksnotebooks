# Databricks notebook source
# MAGIC %md
# MAGIC #### Pre-req: Train model in MLFlow Part 1
# MAGIC For this notebbok to work you should first runt the training notebook with tracking URI set to AML.<br>
# MAGIC See: MLflow with AML Tracking 1 - Train and log <br>

# COMMAND ----------

# MAGIC %md 
# MAGIC **Note:** This notebook expects that you use an Azure Databricks hosted MLflow tracking server. To set up your own tracking server, see the instructions in [MLflow Tracking Servers](https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking-servers) and configure your connection to your tracking server by running [mlflow.set_tracking_uri](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_tracking_uri).

# COMMAND ----------

# MAGIC %md ## Setup

# COMMAND ----------

# MAGIC %pip install mlflow azureml-mlflow

# COMMAND ----------

# MAGIC %md ### Create or load an Azure ML Workspace (interactive login) see notebook 1 for SP based login (automated)

# COMMAND ----------

import azureml.core
from azureml.core import Workspace

# Load the workspace from the saved config file for WS: azureml-ws-databricks
subscription_id = dbutils.secrets.get(scope="databricks", key="subscriptionId")
ws = Workspace(auth="", subscription_id=subscription_id, resource_group="Databricks", workspace_name="azureml-ws-databricks")
print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))

# COMMAND ----------

import mlflow
mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())

# COMMAND ----------

# MAGIC %md
# MAGIC #### New supported way to deploy (if you log in AML that is needed for this method)

# COMMAND ----------

import mlflow
from mlflow.deployments import get_deploy_client

# set the tracking uri as the deployment client
client = get_deploy_client(mlflow.get_tracking_uri())

# set the model path 
model_path = "model"
# model_path = "/Shared/experiments/DiabetesModel/model"

# From Experiment -> Run -> In AML Portal
run_id = "0dbf8538-7391-44c1-8977-201bb66c6824"

# define the model path and the name is the service name
# the model gets registered automatically and a name is autogenerated using the "name" parameter below 
# See: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models#deploy-to-azure-container-instance-aci
# can also provide config for deployment if we need: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models#customize-deployment-configuration
client.create_deployment(model_uri='runs:/{}/{}'.format(run_id, model_path), name="mlflow-test-aci-2")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Only tests below

# COMMAND ----------

'''import azureml
from azureml.core import Workspace

workspace_name = "<workspace-name>"
workspace_location="<workspace-location>"
resource_group = "<resource-group>"
subscription_id = "<subscription-id>"

workspace = Workspace.create(name = workspace_name,
                             location = workspace_location,
                             resource_group = resource_group,
                             subscription_id = subscription_id,
                             exist_ok=True)'''

# COMMAND ----------

# MAGIC %md ### Deprecated - Build an Azure Container Image for model deployment

# COMMAND ----------

# MAGIC %md ### Deprecated - Use MLflow to build a Container Image for the trained model
# MAGIC 
# MAGIC Use the `mlflow.azuereml.build_image` function to build an Azure Container Image for the trained MLflow model. This function also registers the MLflow model with a specified Azure ML workspace. The resulting image can be deployed to Azure Container Instances (ACI) or Azure Kubernetes Service (AKS) for real-time serving.

# COMMAND ----------

# MAGIC %md Specify the run ID associated with an ElasticNet training run from [part 1 of the quickstart guide](https://docs.azuredatabricks.net/spark/latest/mllib/mlflow-tracking.html). You can find a run ID and model path from the experiment run, which can be found on the run details page:
# MAGIC 
# MAGIC ![image](https://docs.azuredatabricks.net/_static/images/mlflow/mlflow-deployment-example-run-info.png)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Deprecated - way of deploying MLFlow stored model to AML ACI

# COMMAND ----------

import mlflow.azureml

model_image, azure_model = mlflow.azureml.build_image(model_uri=model_uri, 
                                                      workspace=ws,
                                                      model_name="model",
                                                      image_name="model",
                                                      description="Sklearn ElasticNet image for predicting diabetes progression",
                                                      synchronous=False)

# COMMAND ----------

model_image.wait_for_creation(show_output=True)

# COMMAND ----------

# MAGIC %md ## Deploy the model to "dev" using [Azure Container Instances (ACI)](https://docs.microsoft.com/en-us/azure/container-instances/)
# MAGIC 
# MAGIC The [ACI platform](https://docs.microsoft.com/en-us/azure/container-instances/) is the recommended environment for staging and developmental model deployments.

# COMMAND ----------

# MAGIC %md ### Create an ACI webservice deployment using the model's Container Image
# MAGIC 
# MAGIC Using the Azure ML SDK, deploy the Container Image for the trained MLflow model to ACI.

# COMMAND ----------

from azureml.core.webservice import AciWebservice, Webservice

dev_webservice_name = "diabetes-model"
dev_webservice_deployment_config = AciWebservice.deploy_configuration()
dev_webservice = Webservice.deploy_from_image(name=dev_webservice_name, image=model_image, deployment_config=dev_webservice_deployment_config, workspace=workspace)

# COMMAND ----------

dev_webservice.wait_for_deployment()

# COMMAND ----------

# MAGIC %md
# MAGIC #### New (not deprecated) way of deploying a model in AML in one step
# MAGIC See: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models

# COMMAND ----------

import os

folder_name = 'diabetes_service_folder'

# Create a folder for the web service files
experiment_folder = './' + folder_name
os.makedirs(experiment_folder, exist_ok=True)
print(folder_name, 'folder created.')

# Set path for scoring script
json_file = os.path.join(experiment_folder,"deployment_config.json")

# COMMAND ----------

# MAGIC %%writefile $json_file
# MAGIC {
# MAGIC  "computeType": "aci",
# MAGIC  "containerResourceRequirements": {"cpu": 1, "memoryInGB": 1},
# MAGIC  "location": "norheurope"
# MAGIC }

# COMMAND ----------

# set the deployment config
# deploy_path = "deployment_config.json"
run_id1 = "8050c3ccc41b4de7b64234f4cd2645c6"
test_config = {'deploy-config-file': json_file}

# set the model path 
model_path = "model"

model_uri='runs:/{}/{}'.format(run_id1, model_path)
print(model_uri)
#client.create_deployment(), model_uri, config=test_config, name="mlflow-test-aci")

# COMMAND ----------

import mlflow

mlflow.set_tracking_uri("databricks")
tracking_url_old = mlflow.get_tracking_uri()
print(tracking_url_old)

mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())
print(mlflow.get_tracking_uri())

# COMMAND ----------

# MAGIC %md
# MAGIC **Note:** To deploy your MLflow model to an Azure Machine Learning web service, your model must be set up with the MLflow Tracking URI to connect with Azure Machine Learning.<br>
# MAGIC This means that we need to store the model in Azure ML so this means that we need to use the Azure ML as back-end all the way when deploying the way below.<br>
# MAGIC Makes sense so that we can keep lineage.<br>
# MAGIC The old way on this "old" notebook was taking the model from MLFlow registry and then build a docker image. Then deployed the image to Azure ACI/AKS. 

# COMMAND ----------

from mlflow.deployments import get_deploy_client

mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())
client = get_deploy_client(mlflow.get_tracking_uri())

# Model URI from Experiment
print(model_uri)

# define the model path and the name is the service name
# the model gets registered automatically and a name is autogenerated using the "name" parameter below 
client.create_deployment(model_uri=model_uri, name="model-from-mlflow-test2")

# COMMAND ----------

# MAGIC %md ## Query the deployed model in "dev"

# COMMAND ----------

# MAGIC %md ### Load diabetes dataset

# COMMAND ----------

from sklearn import datasets
diabetes = datasets.load_diabetes()

# COMMAND ----------

# MAGIC %md ## Create sample input vector

# COMMAND ----------

import pandas as pd
import numpy as np

X = diabetes.data
y = diabetes.target
Y = np.array([y]).transpose()
d = np.concatenate((X, Y), axis=1)
cols = ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'progression']
data = pd.DataFrame(d, columns=cols)
sample = data.drop(["progression"], axis=1).iloc[[0]]
                                                 
query_input = sample.to_json(orient='split')
query_input = eval(query_input)
query_input.pop('index', None)

# COMMAND ----------

# MAGIC %md #### Evaluate the sample input vector by sending an HTTP request
# MAGIC Query the ACI webservice's scoring endpoint by sending an HTTP POST request that contains the input vector.

# COMMAND ----------

import requests
import json

def query_endpoint_example(scoring_uri, inputs, service_key=None):
  headers = {
    "Content-Type": "application/json",
  }
  if service_key is not None:
    headers["Authorization"] = "Bearer {service_key}".format(service_key=service_key)
    
  print("Sending batch prediction request with inputs: {}".format(inputs))
  response = requests.post(scoring_uri, data=json.dumps(inputs), headers=headers)
  preds = json.loads(response.text)
  print("Received response: {}".format(preds))
  return preds

# COMMAND ----------

dev_webservice.scoring_uri

# COMMAND ----------

dev_prediction = query_endpoint_example(scoring_uri=dev_webservice.scoring_uri, inputs=query_input)

# COMMAND ----------

# MAGIC %md ## Deploy the model to production using [Azure Kubernetes Service (AKS)](https://azure.microsoft.com/en-us/services/kubernetes-service/). Do Option 1 or Option 2.

# COMMAND ----------

# MAGIC %md ### Option 1: Create a new AKS cluster
# MAGIC 
# MAGIC If you do not have an active AKS cluster for model deployment, create one using the Azure ML SDK.

# COMMAND ----------

from azureml.core.compute import AksCompute, ComputeTarget

# Use the default configuration (you can also provide parameters to customize this)
prov_config = AksCompute.provisioning_configuration()

aks_cluster_name = "diabetes-cluster" 
# Create the cluster
aks_target = ComputeTarget.create(workspace = workspace, 
                                  name = aks_cluster_name, 
                                  provisioning_configuration = prov_config)

# Wait for the create process to complete
aks_target.wait_for_completion(show_output = True)
print(aks_target.provisioning_state)
print(aks_target.provisioning_errors)

# COMMAND ----------

# MAGIC %md ### Option 2: Connect to an existing AKS cluster
# MAGIC 
# MAGIC If you already have an active AKS cluster running, you can add it to your Workspace using the Azure ML SDK.

# COMMAND ----------

from azureml.core.compute import AksCompute, ComputeTarget

# Get the resource group from https://porta..azure.com -> Find your resource group
resource_group = "<resource-group>"

# Give the cluster a local name
aks_cluster_name = "diabetes-cluster"

# Attatch the cluster to your workgroup
attach_config = AksCompute.attach_configuration(resource_group=resource_group, cluster_name=aks_cluster_name)
aks_target = ComputeTarget.attach(workspace, name="diabetes-compute", attach_config)

# Wait for the operation to complete
aks_target.wait_for_completion(True)
print(aks_target.provisioning_state)
print(aks_target.provisioning_errors)

# COMMAND ----------

# MAGIC %md ### Deploy to the model's image to the specified AKS cluster

# COMMAND ----------

from azureml.core.webservice import Webservice, AksWebservice

# Set configuration and service name
prod_webservice_name = "diabetes-model-prod"
prod_webservice_deployment_config = AksWebservice.deploy_configuration()

# Deploy from image
prod_webservice = Webservice.deploy_from_image(workspace = workspace, 
                                               name = prod_webservice_name,
                                               image = model_image,
                                               deployment_config = prod_webservice_deployment_config,
                                               deployment_target = aks_target)

# COMMAND ----------

# Wait for the deployment to complete
prod_webservice.wait_for_deployment(show_output = True)

# COMMAND ----------

# MAGIC %md ## Query the deployed model in production

# COMMAND ----------

# MAGIC %md #### Evaluate the sample input vector by sending an HTTP request
# MAGIC Query the AKS webservice's scoring endpoint by sending an HTTP POST request that includes the input vector. The production AKS deployment may require an authorization token (service key) for queries. Include this key in the HTTP request header.

# COMMAND ----------

import requests
import json

def query_endpoint_example(scoring_uri, inputs, service_key=None):
  headers = {
    "Content-Type": "application/json",
  }
  if service_key is not None:
    headers["Authorization"] = "Bearer {service_key}".format(service_key=service_key)
    
  print("Sending batch prediction request with inputs: {}".format(inputs))
  response = requests.post(scoring_uri, data=json.dumps(inputs), headers=headers)
  preds = json.loads(response.text)
  print("Received response: {}".format(preds))
  return preds

# COMMAND ----------

prod_scoring_uri = prod_webservice.scoring_uri
prod_service_key = prod_webservice.get_keys()[0] if len(prod_webservice.get_keys()) > 0 else None

# COMMAND ----------

prod_prediction1 = query_endpoint_example(scoring_uri=prod_scoring_uri, service_key=prod_service_key, inputs=query_input)

# COMMAND ----------

# MAGIC %md ## Update the production deployment

# COMMAND ----------

# MAGIC %md ### Build an Azure Container Image for the new model

# COMMAND ----------

run_id2 = "<run-id2>"
model_uri = "runs:/" + run_id2 + "/model"

# COMMAND ----------

import mlflow.azureml

model_image_updated, azure_model_updated = mlflow.azureml.build_image(model_uri=model_uri, 
                                                                      workspace=workspace,
                                                                      model_name="model-updated",
                                                                      image_name="model-updated",
                                                                      description="Sklearn ElasticNet image for predicting diabetes progression",
                                                                      synchronous=False)

# COMMAND ----------

model_image_updated.wait_for_creation(show_output=True)

# COMMAND ----------

# MAGIC %md ### Deploy the new model's image to the AKS cluster
# MAGIC 
# MAGIC Using the [`azureml.core.webservice.AksWebservice.update()`](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.akswebservice?view=azure-ml-py#update) function, replace the deployment's existing model image with the new model image.

# COMMAND ----------

prod_webservice.update(image=model_image_updated)

# COMMAND ----------

prod_webservice.wait_for_deployment(show_output = True)

# COMMAND ----------

# MAGIC %md ### Query the updated model

# COMMAND ----------

prod_prediction2 = query_endpoint_example(scoring_uri=prod_scoring_uri, service_key=prod_service_key, inputs=query_input)

# COMMAND ----------

# MAGIC %md ## Compare the predictions

# COMMAND ----------

print("Run ID: {} Prediction: {}".format(run_id1, prod_prediction1)) 
print("Run ID: {} Prediction: {}".format(run_id2, prod_prediction2))

# COMMAND ----------

# MAGIC %md ## Clean up the deployments

# COMMAND ----------

# MAGIC %md ### Terminate the "dev" ACI webservice
# MAGIC 
# MAGIC Because ACI manages compute resources on your behalf, deleting the "dev" ACI webservice will remove all resources associated with the "dev" model deployment

# COMMAND ----------

dev_webservice.delete()

# COMMAND ----------

# MAGIC %md ### Terminate the production AKS webservice
# MAGIC 
# MAGIC This terminates the real-time serving webservice running on the specified AKS cluster. It **does not** terminate the AKS cluster.

# COMMAND ----------

prod_webservice.delete()

# COMMAND ----------

# MAGIC %md ### Remove the AKS cluster from the Azure ML Workspace
# MAGIC 
# MAGIC If the cluster was created using the Azure ML SDK (see **Option 1: Create a new AKS cluster**), remove it from the Azure ML Workspace will terminate the cluster, including all of its compute resources and deployments.
# MAGIC 
# MAGIC If the cluster was created independently (see **Option 2: Connect to an existing AKS cluster**), it will remain active after removal from the Azure ML Workspace.

# COMMAND ----------

aks_target.delete()

# COMMAND ----------

# MAGIC %md
# MAGIC #### List experiments via mlflow

# COMMAND ----------

#Search experiment runs and get DF back
import mlflow
mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())

df_exp = mlflow.list_experiments()
for exp in df_exp:
  print(exp.name, ":", exp)

# COMMAND ----------

# MAGIC %md
# MAGIC #### List runs for a specific experiment via mlflow

# COMMAND ----------

from pyspark.sql.functions import col, desc
# Load the specific experiment id based on our name
experiment = mlflow.get_experiment_by_name("Diabetes_MLflow_DBX_with_aml_as_tracking_uri")
print("experiment_id:", experiment.experiment_id)
df_runs = mlflow.search_runs(experiment_ids="ce64fe0d-1abd-41b6-92e3-dee03a04c7dc")
display(df_runs)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Get best run based on r2 using Spark DF

# COMMAND ----------

# Pick run with best R2 metric using Spark filtering 
display(spark.createDataFrame(df_runs).orderBy(col("`metrics.r2`"), ascending=False).limit(1))

# COMMAND ----------

# MAGIC %md
# MAGIC #### Get best run based on r2 using Pandas

# COMMAND ----------

df_runs_pandas = mlflow.search_runs(experiment_ids="ce64fe0d-1abd-41b6-92e3-dee03a04c7dc")
idxmax = df_runs_pandas['metrics.r2'].argmax()
df_runs_pandas.iloc[idxmax]
